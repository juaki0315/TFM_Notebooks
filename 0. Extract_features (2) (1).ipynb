{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0. Prepare enviroment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 12:36:26.160722: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-10 12:36:26.790408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Load data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1. Load images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generator from Keras Sequence\n",
    "\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "\n",
    "    def __init__(self, image_paths, labels, batch_size, img_size, shuffle=True, seed=42):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        image_paths : list\n",
    "            List of image paths\n",
    "        labels : array-like\n",
    "            Array of integer labels (e.g., 0, 1, 2, ...).\n",
    "        batch_size : int\n",
    "            Size of the batch.\n",
    "        img_size : tuple\n",
    "            Image size (height, weight).\n",
    "        shuffle : bool, optional\n",
    "            Shuffle after each epoch.\n",
    "        seed : int, optional\n",
    "            Random seed.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = np.array(labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.n_classes = len(np.unique(self.labels))\n",
    "        if self.n_classes == 2:\n",
    "            self.n_classes = 1\n",
    "            \n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.indexes = np.arange(len(self.image_paths))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Number of batches per epoch\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Generating indices for the current epoch\n",
    "        start = index * self.batch_size\n",
    "        end = (index + 1) * self.batch_size\n",
    "        batch_indexes = self.indexes[start:end]\n",
    "\n",
    "        # Batch creation\n",
    "        batch_image_paths = [self.image_paths[i] for i in batch_indexes]\n",
    "        batch_labels = [self.labels[i] for i in batch_indexes]\n",
    "        \n",
    "        X, y = self.__data_generation(batch_image_paths, batch_labels)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch\n",
    "        if self.shuffle:\n",
    "            np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "    def __data_generation(self, batch_image_paths, batch_labels):\n",
    "        \n",
    "        # Initialize arrays to store batch data\n",
    "        X = np.empty((len(batch_image_paths), self.img_size[0], self.img_size[1], 3), dtype=np.float32)\n",
    "        y = np.empty((len(batch_image_paths),), dtype=int)\n",
    "    \n",
    "        # Iterate over each .npy image in the batch\n",
    "        for i, path in enumerate(batch_image_paths):\n",
    "            # Load preprocessed image from .npy file\n",
    "            X_ = np.load(path)\n",
    "            X[i] = X_\n",
    "            y[i] = batch_labels[i]\n",
    "    \n",
    "        # Convert labels to one-hot encoding if there is more than two classes\n",
    "        if self.n_classes > 2:\n",
    "            y = to_categorical(y, num_classes=self.n_classes)\n",
    "            \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsets\n",
    "subset_train = ['TC_Canon_RAD_HURJC_train', 'TC_Canon_CZC_HURJC_train', 'TC_Canon_MIN_HURJC_train']\n",
    "subsets_train_name = 'TC_Canon_CZC_RAD_MIN_HURJC_train'\n",
    "subset_test  = ['TC_Canon_RAD_HURJC_test', 'TC_Canon_CZC_HURJC_test', 'TC_Canon_MIN_HURJC_test']\n",
    "subsets_test_name = 'TC_Canon_CZC_RAD_MIN_HURJC_test'\n",
    "\n",
    "# Ruta base\n",
    "subsets_path = 'big_volume/subsets_v0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar rutas de imágenes y etiquetas en train y test\n",
    "images_train_path = []\n",
    "images_test_path = []\n",
    "labels_train = []\n",
    "labels_test = []\n",
    "\n",
    "for s in subset_train:\n",
    "    subset_path = os.path.join(subsets_path, s)\n",
    "    df = pd.read_csv(os.path.join(subsets_path, s + '.csv'))\n",
    "    label_dict = dict(zip(df['rx_cod'], df['label_CalTend']))\n",
    "    \n",
    "    for img_name in os.listdir(subset_path):\n",
    "        img_path = os.path.join(subset_path, img_name)\n",
    "        if os.path.exists(img_path):\n",
    "            images_train_path.append(img_path)\n",
    "            labels_train.append(label_dict[os.path.splitext(img_name)[0]])\n",
    "\n",
    "for s in subset_test:\n",
    "    subset_path = os.path.join(subsets_path, s)\n",
    "    df = pd.read_csv(os.path.join(subsets_path, s + '.csv'))\n",
    "    label_dict = dict(zip(df['rx_cod'], df['label_CalTend']))\n",
    "    \n",
    "    for img_name in os.listdir(subset_path):\n",
    "        img_path = os.path.join(subset_path, img_name)\n",
    "        if os.path.exists(img_path):\n",
    "            images_test_path.append(img_path)\n",
    "            labels_test.append(label_dict[os.path.splitext(img_name)[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros del generador\n",
    "batch_size = 32\n",
    "img_size = (512, 512)  # <-- AQUÍ defines el tamaño correcto\n",
    "\n",
    "# Crear generadores\n",
    "train_generator = DataGenerator(images_train_path, labels_train, batch_size, img_size, shuffle=True)\n",
    "test_generator  = DataGenerator(images_test_path,  labels_test,  batch_size, img_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch shape: (32, 512, 512, 3)\n",
      "y_batch shape: (32,)\n",
      "Example labels: [1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "X_batch, y_batch = train_generator[0]\n",
    "print(f'X_batch shape: {X_batch.shape}')\n",
    "print(f'y_batch shape: {y_batch.shape}')\n",
    "print(f'Example labels: {y_batch[:5]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 4268\n",
      "Labels: (4268,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Subsets\n",
    "subset_train = ['TC_Canon_RAD_HURJC_train', 'TC_Canon_CZC_HURJC_train', 'TC_Canon_MIN_HURJC_train']\n",
    "subsets_train_name = 'TC_Canon_CZC_RAD_MIN_HURJC_train'\n",
    "subset_test  = ['TC_Canon_RAD_HURJC_test', 'TC_Canon_CZC_HURJC_test', 'TC_Canon_MIN_HURJC_test']\n",
    "subsets_test_name = 'TC_Canon_CZC_RAD_MIN_HURJC_test'\n",
    "\n",
    "# Ruta base\n",
    "subsets_path = 'big_volume/subsets_v0/'\n",
    "\n",
    "# Inicializar listas\n",
    "images_path = []\n",
    "labels = []\n",
    "\n",
    "# Procesar todos los subsets (entrenamiento + test)\n",
    "subsets = subset_train + subset_test\n",
    "\n",
    "for s in subsets:\n",
    "    # Carpeta de imágenes\n",
    "    subset_path = os.path.join(subsets_path, s)\n",
    "    images_subset = [os.path.join(subset_path, image_name) for image_name in os.listdir(subset_path)]\n",
    "\n",
    "    # Filtrar imágenes que realmente existen\n",
    "    images_subset = [img_path for img_path in images_subset if os.path.exists(img_path)]\n",
    "    images_path.extend(images_subset)\n",
    "\n",
    "    # Metadata\n",
    "    df = pd.read_csv(os.path.join(subsets_path, s + '.csv'))\n",
    "\n",
    "    # Extraer etiquetas\n",
    "    label_dict = dict(zip(df['rx_cod'], df['label_CalTend']))\n",
    "    labels_subset = [\n",
    "        label_dict[\n",
    "            os.path.splitext(os.path.basename(img_path))[0]\n",
    "        ]\n",
    "        for img_path in images_subset\n",
    "    ]\n",
    "    labels.extend(labels_subset)\n",
    "\n",
    "# Convertir a array\n",
    "images_path = np.array(images_path)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Resumen\n",
    "print('Images:', len(images_path))\n",
    "print('Labels:', y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2. Load metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (3768, 20)\n",
      "label_CalTend\n",
      "0    1884\n",
      "1    1884\n",
      "Name: count, dtype: int64\n",
      "Test set: (500, 20)\n",
      "label_CalTend\n",
      "1    250\n",
      "0    250\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cargar y unir CSVs de train\n",
    "df_train = pd.concat([\n",
    "    pd.read_csv(os.path.join(subsets_path, s + '.csv'))\n",
    "    for s in subset_train\n",
    "], ignore_index=True)\n",
    "\n",
    "# Cargar y unir CSVs de test\n",
    "df_test = pd.concat([\n",
    "    pd.read_csv(os.path.join(subsets_path, s + '.csv'))\n",
    "    for s in subset_test\n",
    "], ignore_index=True)\n",
    "\n",
    "# Opcional: imprime resumen rápido\n",
    "print(\"Train set:\", df_train.shape)\n",
    "print(df_train['label_CalTend'].value_counts())\n",
    "\n",
    "print(\"Test set:\", df_test.shape)\n",
    "print(df_test['label_CalTend'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels\n",
    "def extract_metadata (df, images_path):\n",
    "    \n",
    "    label_dict     = dict(zip(df['rx_cod'], df['label_CalTend']))\n",
    "    sex_dict       = dict(zip(df['rx_cod'], df['sex']))\n",
    "    birthdate_dict = dict(zip(df['rx_cod'], df['birthdate']))\n",
    "    date_rx_dict   = dict(zip(df['rx_cod'], df['date_rx']))\n",
    "\n",
    "    rx_cod    = np.array([os.path.split(images_path)[1][:-4] for images_path in images_path])\n",
    "    y         = np.array([label_dict     [os.path.split(images_path)[1][:-4]] for images_path in images_path])\n",
    "    sex       = np.array([sex_dict       [os.path.split(images_path)[1][:-4]] for images_path in images_path])\n",
    "    birthdate = np.array([birthdate_dict [os.path.split(images_path)[1][:-4]] for images_path in images_path])\n",
    "    date_rx   = np.array([date_rx_dict   [os.path.split(images_path)[1][:-4]] for images_path in images_path])\n",
    "\n",
    "    return rx_cod, y, sex, birthdate, date_rx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_shape_train: (3768,)\n",
      "y_shape_test:  (500,)\n"
     ]
    }
   ],
   "source": [
    "rx_cod_train, y_train, sex_train, birthdate_train, date_rx_train = extract_metadata (df_train, images_train_path)\n",
    "rx_cod_test,  y_test,  sex_test,  birthdate_test,  date_rx_test  = extract_metadata (df_test,  images_test_path)\n",
    "\n",
    "print(f'y_shape_train: {y_train.shape}')\n",
    "print(f'y_shape_test:  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Load model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 12:36:32.180396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-10 12:36:32.241127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-10 12:36:32.244082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-10 12:36:32.248928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-10 12:36:32.251801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-10 12:36:32.254451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-10 12:36:32.467848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-10 12:36:32.469671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-10 12:36:32.471559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-10 12:36:32.473169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 16, 16, 512)       20024384  \n",
      "                                                                 \n",
      " global_max_pooling2d (Glob  (None, 512)               0         \n",
      " alMaxPooling2D)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20024897 (76.39 MB)\n",
      "Trainable params: 20024897 (76.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# EJEMPLO MODELO CON \"NESTED BASE_MODEL\"\n",
    "# Cargar el modelo\n",
    "model_path = 'big_volume/Models_v0/EXP1/TC_Canon_CZC_RAD_MIN_HURJC_train/model_conv_VGG19_freeze_False_top_m_GMP_neurons_last_layer_1_optimizer_SGD_loss_function_binary_crossentropy_activation_s_initializer_seed_42_lr_0.005_epochs_50_batch_size_32_patience_15_subset_TC_Canon_CZC_RAD_MIN_HURJC_train/FOLD_4/best_model_epoch_22_val_loss_0.5731_val_acc_0.8486.h5'\n",
    "model = load_model(model_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.Extract features of last conv + GMP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 12:36:34.296751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 18s 18s/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 13s 13s/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Asegurar índices únicos para acceso rápido\n",
    "df_train = df_train.set_index('rx_cod')\n",
    "df_test  = df_test.set_index('rx_cod')\n",
    "\n",
    "# Crear diccionarios de metadatos\n",
    "metadata_train = df_train[['sex', 'birthdate', 'date_rx', 'station_name']].to_dict(orient='index')\n",
    "metadata_test  = df_test[['sex', 'birthdate', 'date_rx', 'station_name']].to_dict(orient='index')\n",
    "\n",
    "# Crear el extractor de características desde VGG19\n",
    "conv_model = model.get_layer('vgg19')  # Asegúrate de que existe\n",
    "last_conv_layer = conv_model.get_layer('block5_conv3')\n",
    "feature_extractor = Model(inputs=conv_model.input, outputs=last_conv_layer.output)\n",
    "\n",
    "# Función de extracción por batches\n",
    "def extract_features(generator, metadata_dict):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    rx_cod_list = []\n",
    "    sex_list = []\n",
    "    birthdate_list = []\n",
    "    date_rx_list = []\n",
    "    station_rx_list = []\n",
    "\n",
    "    for i in range(len(generator)):\n",
    "        X_batch, y_batch = generator[i]\n",
    "\n",
    "        # Extraer feature maps y aplicar pooling\n",
    "        feature_maps = feature_extractor.predict(X_batch)\n",
    "        pooled_features = GlobalMaxPooling2D()(feature_maps).numpy()\n",
    "\n",
    "        feature_list.append(pooled_features)\n",
    "        label_list.append(y_batch)\n",
    "\n",
    "        # Obtener rx_cod del batch\n",
    "        batch_indexes = generator.indexes[i * generator.batch_size : (i + 1) * generator.batch_size].tolist()\n",
    "        batch_rx_cod = [os.path.splitext(os.path.basename(generator.image_paths[j]))[0] for j in batch_indexes]\n",
    "        rx_cod_list.extend(batch_rx_cod)\n",
    "\n",
    "        # Añadir metadatos\n",
    "        for cod in batch_rx_cod:\n",
    "            info = metadata_dict.get(cod, {'sex': None, 'birthdate': None, 'date_rx': None, 'station_name':None})\n",
    "            sex_list.append(info['sex'])\n",
    "            birthdate_list.append(info['birthdate'])\n",
    "            date_rx_list.append(info['date_rx'])\n",
    "            station_rx_list.append(info['station_name'])\n",
    "\n",
    "    features = np.vstack(feature_list)\n",
    "    labels = np.concatenate(label_list).flatten()\n",
    "    return features, labels, rx_cod_list, sex_list, birthdate_list, date_rx_list, station_rx_list\n",
    "\n",
    "# Extraer características de train y test\n",
    "features_train, y_train, rx_cod_train, sex_train, birthdate_train, date_rx_train, station_rx_train = extract_features(train_generator, metadata_train)\n",
    "features_test,  y_test,  rx_cod_test,  sex_test,  birthdate_test,  date_rx_test, station_rx_test  = extract_features(test_generator,  metadata_test)\n",
    "\n",
    "# Crear DataFrames\n",
    "column_names = [f'feature_{i+1}' for i in range(features_train.shape[1])]\n",
    "df_features_train = pd.DataFrame(features_train, columns=column_names, index=rx_cod_train)\n",
    "df_features_test  = pd.DataFrame(features_test,  columns=column_names, index=rx_cod_test)\n",
    "\n",
    "# Añadir variables\n",
    "df_features_train['label']      = y_train\n",
    "df_features_test['label']       = y_test\n",
    "\n",
    "df_features_train['sex']        = sex_train\n",
    "df_features_test['sex']         = sex_test\n",
    "\n",
    "df_features_train['birthdate']  = birthdate_train\n",
    "df_features_test['birthdate']   = birthdate_test\n",
    "\n",
    "df_features_train['date_rx']    = date_rx_train\n",
    "df_features_test['date_rx']     = date_rx_test\n",
    "\n",
    "df_features_train['station_name']    = station_rx_train\n",
    "df_features_test['station_name']     = station_rx_test\n",
    "\n",
    "\n",
    "# Calcular edad\n",
    "def calcular_edad(birthdate_str, date_rx_str):\n",
    "    try:\n",
    "        birth = datetime.strptime(birthdate_str, '%Y-%m-%d')\n",
    "        rx    = datetime.strptime(date_rx_str, '%Y-%m-%d')\n",
    "        return (rx - birth).days // 365\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_features_train['age'] = [\n",
    "    calcular_edad(bd, drx) for bd, drx in zip(df_features_train['birthdate'], df_features_train['date_rx'])\n",
    "]\n",
    "df_features_test['age'] = [\n",
    "    calcular_edad(bd, drx) for bd, drx in zip(df_features_test['birthdate'], df_features_test['date_rx'])\n",
    "]\n",
    "\n",
    "# Guardar en CSV\n",
    "df_features_train.to_csv('Machine_Learning/TC_Canon_CZC_RAD_MIN_HURJC_train_features.csv')\n",
    "df_features_test.to_csv('Machine_Learning/TC_Canon_CZC_RAD_MIN_HURJC_test_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
